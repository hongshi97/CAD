{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version : 3.8.10 (default, May 26 2023, 14:05:08) \n",
      "[GCC 9.4.0]\n",
      "Torch Version : 2.1.0+cu118\n",
      "Transformers Version : 4.35.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Python Version : {sys.version}\")\n",
    "print(f\"Torch Version : {torch.__version__}\")\n",
    "print(f\"Transformers Version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(1002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAD:\n",
    "    def __init__(self, model_name: str, device: Union[int,str] = 0):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map=device, use_cache=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if model_name.startswith('huggyllama'): # add [PAD] token to tokenizer if model_name is huggyllama, because huggyllama doesn't have a pad token\n",
    "            special_tokens_dict = {'pad_token': '[PAD]'}\n",
    "            self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "\n",
    "    def _top_p_sampling(self, \n",
    "                        logits: torch.Tensor, \n",
    "                        top_p: float = 0.9, \n",
    "                        filter_value: float = -float(\"Inf\"), \n",
    "                        min_tokens_to_keep: int = 1\n",
    "                        ) -> torch.Tensor :\n",
    "\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        \n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep - 1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        \n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def _top_k_sampling(self, \n",
    "                        logits: torch.Tensor, \n",
    "                        top_k: int = 20, \n",
    "                        filter_value: float = -float(\"Inf\"), \n",
    "                        min_tokens_to_keep: int = 1\n",
    "                        ) -> torch.Tensor :\n",
    "\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None] # * logit 값이 Top-k의 토큰 중 가장 작은 값보다 작은 토큰의 인덱스 반환 \n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def predict_next_token(self, \n",
    "                           logits: torch.Tensor, \n",
    "                           decoding_strategy: str, \n",
    "                           top_p: float, \n",
    "                           top_k: int, \n",
    "                           use_repetition_penalty: bool, \n",
    "                           repetition_penalty_value: float, \n",
    "                           generated_tokens: List[set] = None\n",
    "                           ) -> torch.Tensor :\n",
    "\n",
    "        # * Repetitin Penalty 참고 코드 : https://huggingface.co/transformers/v2.11.0/_modules/transformers/modeling_utils.html#PreTrainedModel.enforce_repetition_penalty_\n",
    "        if use_repetition_penalty:\n",
    "            assert repetition_penalty_value >= 1.0, \"Repetition penalty must be >= 1.\"\n",
    "            mask = torch.zeros_like(logits)\n",
    "            for i, token_set in enumerate(generated_tokens):\n",
    "                mask[i, list(token_set)] = 1.0\n",
    "            penalty = torch.where(mask == 1.0, repetition_penalty_value, 1.0) # generated_tokens에 있는 토큰들은 penalty를 repetition_penalty_value로, 없는 토큰들은 1.0(현상 유지)으로 설정\n",
    "            logits *= torch.where(logits < 0, penalty, 1.0/penalty) # if logit is smaller than 0, multiply with penalty, else divide by penalty\n",
    "                                                                    # 음수 * (1보다 큰 양수) -> 더 작은 값, 양수 / (1보다 큰 양수) -> 더 작은 값 => generated_tokens에 있는 Token의 로짓값 감소시킴\n",
    "        \n",
    "        if decoding_strategy == 'top_p':\n",
    "            assert top_p is not None, \"top_p must be provided for top_p sampling\"\n",
    "            logits = self._top_p_sampling(logits, top_p)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "        elif decoding_strategy == 'top_k':\n",
    "            assert top_k is not None, \"top_k must be provided for top_k sampling\"\n",
    "            logits = self._top_k_sampling(logits, top_k)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "        elif decoding_strategy == 'greedy':\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return next_token\n",
    "\n",
    "\n",
    "    def generate(self, \n",
    "                input_texts: List[str], \n",
    "                contexts: Optional[List[str]] = None, \n",
    "                use_context_aware: bool = True,\n",
    "                alpha: float = 0.5,\n",
    "                max_length: int = 256,\n",
    "                decoding_strategy: str = 'top_p',\n",
    "                top_p_value: float = 0.9,\n",
    "                top_k_value: int = 20,\n",
    "                use_repetition_penalty: bool = False, \n",
    "                repetition_penalty_value: float = 1.0,\n",
    "                ) -> List[List[int]]:\n",
    "\n",
    "        # Tokenize 'input_texts' and create attention masks\n",
    "        tokenized_inputs = self.tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        input_ids = tokenized_inputs['input_ids']\n",
    "        attention_mask = tokenized_inputs['attention_mask']\n",
    "\n",
    "        # Tokenize 'contexts' after concatenating with 'input_ids' if 'contexts' is not None\n",
    "        if contexts and use_context_aware:\n",
    "            inputs_with_contexts = [context + self.tokenizer.eos_token + input_text for context, input_text in zip(contexts, input_texts)]\n",
    "            tokenized_inputs_with_contexts = self.tokenizer(inputs_with_contexts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "            input_ids_with_contexts = tokenized_inputs_with_contexts['input_ids']\n",
    "            attention_mask_with_contexts = tokenized_inputs_with_contexts['attention_mask']\n",
    "        else:\n",
    "            input_ids_with_contexts = input_ids\n",
    "            attention_mask_with_contexts = attention_mask\n",
    "\n",
    "        # Initialize variables for generation loop\n",
    "        cur_len = 0\n",
    "        batch_size = len(input_ids)\n",
    "        unfinished_sents = input_ids_with_contexts.new(batch_size).fill_(1)\n",
    "        sent_lengths = input_ids_with_contexts.new(batch_size).fill_(max_length)\n",
    "\n",
    "        generated_tokens = [[] for _ in range(batch_size)] # e.g., [[4132, 102, 29402], [2378, 7893, 23001]]\n",
    "\n",
    "        # Generate tokens\n",
    "        with torch.no_grad():\n",
    "            while cur_len < max_length:\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                next_token_logits = outputs.logits[:, -1, :] # (batch_size, vocab_size)\n",
    "\n",
    "                # * Context-aware Decoding\n",
    "                if contexts and use_context_aware:\n",
    "                    outputs_with_contexts = self.model(input_ids_with_contexts, attention_mask=attention_mask_with_contexts)\n",
    "                    next_token_logits_with_contexts = outputs_with_contexts.logits[:, -1, :]\n",
    "                    next_token_logits = (1 + alpha) * next_token_logits_with_contexts - alpha * next_token_logits\n",
    "\n",
    "                # Predict next token according to decoding strategy\n",
    "                next_token = self.predict_next_token(logits=next_token_logits, \n",
    "                                                    decoding_strategy=decoding_strategy, \n",
    "                                                    top_p=top_p_value, \n",
    "                                                    top_k=top_k_value, \n",
    "                                                    use_repetition_penalty=use_repetition_penalty, \n",
    "                                                    repetition_penalty_value=repetition_penalty_value, \n",
    "                                                    generated_tokens=[set(tokens) for tokens in generated_tokens])\n",
    "\n",
    "                # Handle EOS token and padding\n",
    "                if self.tokenizer.eos_token_id is not None:\n",
    "                    tokens_to_add = next_token * unfinished_sents + (self.tokenizer.pad_token_id) * (1 - unfinished_sents)\n",
    "                else:\n",
    "                    tokens_to_add = next_token\n",
    "\n",
    "                # Update input_ids and attention masks for the next forward pass\n",
    "                input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "                attention_mask = torch.cat([attention_mask, unfinished_sents.unsqueeze(-1)], dim=-1)\n",
    "                input_ids_with_contexts = torch.cat([input_ids_with_contexts, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "                attention_mask_with_contexts = torch.cat([attention_mask_with_contexts, unfinished_sents.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "                cur_len += 1\n",
    "\n",
    "                # Update generated tokens and check for completion\n",
    "                for i, token in enumerate(tokens_to_add.tolist()):\n",
    "                    if unfinished_sents[i] == 1:\n",
    "                        generated_tokens[i].append(token)\n",
    "\n",
    "                # Check for sentences that are finished\n",
    "                if self.tokenizer.eos_token_id is not None:\n",
    "                    eos_in_sents = tokens_to_add == self.tokenizer.eos_token_id\n",
    "                    is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\n",
    "                    sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, cur_len)\n",
    "                    unfinished_sents.mul_((~eos_in_sents).long())\n",
    "\n",
    "                # Break if all sentences are finished : stop when there is a EOS token in each sentence, or if we exceed the maximul length\n",
    "                if unfinished_sents.max() == 0:\n",
    "                    break\n",
    "\n",
    "        # Return the generated tokens\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.77s/it]\n"
     ]
    }
   ],
   "source": [
    "cad_model = CAD(model_name=\"huggyllama/llama-13b\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 1 : Context-aware Decoding 사용 전/후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = ['Write a quote that ends in the word \"early\":']\n",
    "input_texts = ['Better late than']\n",
    "\n",
    "outputs = cad_model.generate(\n",
    "                            input_texts=input_texts,\n",
    "                            use_context_aware=True,\n",
    "                            contexts=contexts,\n",
    "                            max_length=20,\n",
    "                            alpha=0.5,\n",
    "                            decoding_strategy='top_p',\n",
    "                            top_p_value=0.9,\n",
    "                            use_repetition_penalty=True,\n",
    "                            repetition_penalty_value=1.5,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early.<s>Write a quote that ends in the word \"prove\":</red>T\n"
     ]
    }
   ],
   "source": [
    "print(cad_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID : 4688 | Token: early\n",
      "Token ID : 19423 | Token: .<\n",
      "Token ID : 29879 | Token: s\n",
      "Token ID : 29958 | Token: >\n",
      "Token ID : 6113 | Token: Write\n",
      "Token ID : 263 | Token: a\n",
      "Token ID : 14978 | Token: quote\n",
      "Token ID : 393 | Token: that\n",
      "Token ID : 10614 | Token: ends\n",
      "Token ID : 297 | Token: in\n",
      "Token ID : 278 | Token: the\n",
      "Token ID : 1734 | Token: word\n",
      "Token ID : 376 | Token: \"\n",
      "Token ID : 771 | Token: pro\n",
      "Token ID : 345 | Token: ve\n",
      "Token ID : 1115 | Token: \":\n",
      "Token ID : 829 | Token: </\n",
      "Token ID : 1127 | Token: red\n",
      "Token ID : 29958 | Token: >\n",
      "Token ID : 29911 | Token: T\n"
     ]
    }
   ],
   "source": [
    "for i in outputs[0]:\n",
    "    print(f'Token ID : {i} | Token: {cad_model.tokenizer.decode(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 2: Repetition Penalty 사용 전/후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition Penalty : True\n",
      "early.</s>Write a quote that ends in the word \"early\":</s>Better late than earl...\n",
      "<p class=\"MsoNormal\">\"The best way to predict your future is by creating it.\" - Abraham\n",
      "\n",
      "Repetition Penalty : False\n",
      "early.</s>Write a quote that ends in the word \"early\":</s>Better late than early.</s>\n",
      "Write a quote that ends in the word \"early\":</s>Better late than early.</s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contexts = ['Write a quote that ends in the word \"early\":']\n",
    "input_texts = ['Better late than']\n",
    "\n",
    "for bool in [True, False]:\n",
    "    outputs = cad_model.generate(\n",
    "                                input_texts=input_texts,\n",
    "                                use_context_aware=True,\n",
    "                                contexts=contexts,\n",
    "                                max_length=50,\n",
    "                                alpha=0.5,\n",
    "                                decoding_strategy='greedy',\n",
    "                                top_p_value=0.9,\n",
    "                                use_repetition_penalty=bool,\n",
    "                                repetition_penalty_value=1.5,\n",
    "                                )\n",
    "    print(f\"Repetition Penalty : {bool}\")\n",
    "    print(cad_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 3 : alpha 값 변경 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : -0.5 | never.\n",
      "When I was a teenager, my mom used to call me late at night \n",
      "\n",
      "alpha : 0.5 | early.<s>Write a quote that ends in the word \"man\":</s>\"Nob \n",
      "\n",
      "alpha : 1 | early</s>Better earler than never.</textarea></p><button onclick=\"quoteResults \n",
      "\n",
      "alpha : 3 | ear</s><br>Write another quote that ends in the workd \"dark\":<S \n",
      "\n",
      "alpha : 9 | ear</s>.Write quote that ends ion \"slow\": </Slo>Life inthen city \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in [-0.5, 0.5, 1, 3, 9]:\n",
    "    outputs = cad_model.generate(\n",
    "                                input_texts=input_texts,\n",
    "                                use_context_aware=True,\n",
    "                                contexts=contexts,\n",
    "                                max_length=20,\n",
    "                                alpha=alpha,\n",
    "                                decoding_strategy='top_p',\n",
    "                                top_p_value=0.9,\n",
    "                                use_repetition_penalty=True,\n",
    "                                repetition_penalty_value=1.5,\n",
    "                                )\n",
    "\n",
    "    print(f'alpha : {alpha} | {cad_model.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 인풋 작동 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = ['Write a quote that ends in the word \"early\":', 'Translate the following sentence into English:']\n",
    "input_texts = ['Better late than', 'Je suis un homme']\n",
    "\n",
    "outputs = cad_model.generate(\n",
    "                            input_texts=input_texts,\n",
    "                            use_context_aware=True,\n",
    "                            contexts=contexts,\n",
    "                            max_length=20,\n",
    "                            alpha=0.5,\n",
    "                            decoding_strategy='top_p',\n",
    "                            top_p_value=0.9,\n",
    "                            use_repetition_penalty=True,\n",
    "                            repetition_penalty_value=1.5,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early.</s>\\nWrite a quote that ends in the word \"early\":</o:',\n",
       " '.</s>\\nThe sentence may be changed in two different ways:<p></stress>&']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cad_model.tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID : 4688 | Token: early\n",
      "Token ID : 21106 | Token: .</\n",
      "Token ID : 29879 | Token: s\n",
      "Token ID : 29958 | Token: >\n",
      "Token ID : 13 | Token: \n",
      "\n",
      "Token ID : 6113 | Token: Write\n",
      "Token ID : 263 | Token: a\n",
      "Token ID : 14978 | Token: quote\n",
      "Token ID : 393 | Token: that\n",
      "Token ID : 10614 | Token: ends\n",
      "Token ID : 297 | Token: in\n",
      "Token ID : 278 | Token: the\n",
      "Token ID : 1734 | Token: word\n",
      "Token ID : 376 | Token: \"\n",
      "Token ID : 799 | Token: ear\n",
      "Token ID : 368 | Token: ly\n",
      "Token ID : 1115 | Token: \":\n",
      "Token ID : 829 | Token: </\n",
      "Token ID : 29877 | Token: o\n",
      "Token ID : 29901 | Token: :\n"
     ]
    }
   ],
   "source": [
    "for i in outputs[0]:\n",
    "    print(f'Token ID : {i} | Token: {cad_model.tokenizer.decode(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID : 21106 | Token: .</\n",
      "Token ID : 29879 | Token: s\n",
      "Token ID : 29958 | Token: >\n",
      "Token ID : 13 | Token: \n",
      "\n",
      "Token ID : 1576 | Token: The\n",
      "Token ID : 10541 | Token: sentence\n",
      "Token ID : 1122 | Token: may\n",
      "Token ID : 367 | Token: be\n",
      "Token ID : 3939 | Token: changed\n",
      "Token ID : 297 | Token: in\n",
      "Token ID : 1023 | Token: two\n",
      "Token ID : 1422 | Token: different\n",
      "Token ID : 5837 | Token: ways\n",
      "Token ID : 29901 | Token: :\n",
      "Token ID : 29966 | Token: <\n",
      "Token ID : 29886 | Token: p\n",
      "Token ID : 2565 | Token: ></\n",
      "Token ID : 710 | Token: str\n",
      "Token ID : 404 | Token: ess\n",
      "Token ID : 19250 | Token: >&\n"
     ]
    }
   ],
   "source": [
    "for i in outputs[1]:\n",
    "    print(f'Token ID : {i} | Token: {cad_model.tokenizer.decode(i)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
